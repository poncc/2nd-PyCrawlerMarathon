{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 爬蟲筆記\n",
    "##poncc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repr:將函數轉換成編譯器讀取的形式\n",
    "\n",
    "str[1:2]:從1個字元開始,第二個字元結束\n",
    "\n",
    "str:[1:-1:2]:從第一個字元開始,倒數第二個字元結束,且每2個字元抓取一次\n",
    "\n",
    "header抓取：day018\n",
    "\n",
    "find_all,find,dict用法:day019~day022\n",
    "\n",
    "yeild:本身是一種iterator，可以使用next(x)來指向下一個參，參考test_yield\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter自動完成函數\n",
    "\n",
    "1.關閉jupyter\n",
    "2.pip install jupyter_contrib_nbextensions\n",
    "\n",
    "3.jupyter contrib nbextension install --user --skip-running-check\n",
    "\n",
    "3.開啟jupyter\n",
    "\n",
    "4.Nbextensions -> 勾選Hinterland\n",
    "\n",
    "5.Parameters的參數，可以空置延遲多久後跳出函數\n",
    "\n",
    "6.編碼觀念：https://www.ptt.cc/bbs/Python/M.1380034106.A.553.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業筆記 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "001:<br>\n",
    "資料來源與取得<br>\n",
    "開放資料<br>\n",
    "資料儲存格式<br>\n",
    "Python 存取檔案\n",
    "<br><br>\n",
    "    \n",
    "002:<br>\n",
    "了解 csv 檔案格式與內容<br>\n",
    "能夠利用套件存取 csv 格式的檔案\n",
    "<br><br>\n",
    "\n",
    "003:<br>\n",
    "了解 xml 檔案格式與內容<br>\n",
    "能夠利用套件存取 xml 格式的檔案\n",
    "<br><br>\n",
    "\n",
    "004:<br>\n",
    "了解 Server Client 的架構與溝通方法<br>\n",
    "知道 HTTP Request & Response 的內容<br>\n",
    "什麼是 API？如何用 Python 程式存取 API 資料\n",
    "<br><br>\n",
    "\n",
    "005:<br>\n",
    "了解 Dcard API 使用方式與回傳內容<br>\n",
    "撰寫程式存取 API 且解析 JSON 格式資料\n",
    "<br><br>\n",
    "\n",
    "006:<br>\n",
    "了解知乎 API 使用方式與回傳內容<br>\n",
    "撰寫程式存取 API 且添加標頭\n",
    "<br><br>\n",
    "\n",
    "007:<br>\n",
    "認識靜態網頁的溝通架構與運作原理<br>\n",
    "HTML、CSS、JavaScript 在網頁中扮演的角色<br>\n",
    "網頁中定位資料的方法\n",
    "<br><br>\n",
    "\n",
    "008:<br>\n",
    "了解靜態網頁的資料爬蟲策略<br>\n",
    "認識適用於靜態網頁爬蟲的相關套件工具：Request<br>\n",
    "認識適用於靜態網頁爬蟲的相關套件工具：BeatifulSoup\n",
    "<br><br>\n",
    "\n",
    "009:<br>\n",
    "以正確的副檔名下載網頁中的圖片 範例網頁:https://www.ptt.cc/bbs/Beauty/M.1556291059.A.75A.html<br>\n",
    "Note：因為 PTT 會詢問「是否滿 18 歲」，這邊可以用 cookies 繞過<br>\n",
    "requests.get(URL, cookies={'over18': '1'}<br>\n",
    "<br><br>\n",
    "\n",
    "010:<br>\n",
    "利用 Grab 套件的存取 HTML 資源<br>\n",
    "利用 PyQuery 套件的解析 HTML 格式\n",
    "<br><br>\n",
    "\n",
    "011:<br>\n",
    "正規表達式練習\n",
    "<br><br>\n",
    "\n",
    "012:<br>\n",
    "能夠利用 Request + BeatifulSour 撰寫爬蟲，並存放到合適的資料結構\n",
    "<br><br>\n",
    "\n",
    "013:ptt<br>\n",
    "能夠利用 Request + BeatifulSour 撰寫爬蟲，並存放到合適的資料結構\n",
    "<br><br>\n",
    "\n",
    "014:<br>\n",
    "練習爬取電影放映資訊。必須逐步獲取電影的代號、放映地區、放映日期後，再送出查詢給伺服器\n",
    "<br><br>\n",
    "\n",
    "015:<br>\n",
    "練習爬取台幣對其他貨幣匯率資料\n",
    "<br><br>\n",
    "\n",
    "016:<br>\n",
    "練習是從Wikipedia中爬取文章。先定義一個搜尋的關鍵字，擷取該關鍵字詞的文章。<br>\n",
    "func : dict.update\n",
    "<br><br>\n",
    "\n",
    "017:簡答<br>\n",
    "了解動態網頁的資料爬蟲策略<br>\n",
    "知道非同步網頁載入機制（Ajax）<br>\n",
    "學習兩種對應動態網頁爬蟲的的策略\n",
    "<br><br>\n",
    "\n",
    "018:<br>\n",
    "知道瀏覽器的開發者工具<br>\n",
    "能夠使用瀏覽器的開發者工具觀察資料\n",
    "func : header\n",
    "<br><br>\n",
    "\n",
    "019:<br>\n",
    "了解 Selenium 用於動態網頁爬蟲的原理<br>\n",
    "能夠使用 Selenium 撰寫動態網頁爬蟲<br>\n",
    "func : webdriver.Chrome,select\n",
    "<br><br>\n",
    "\n",
    "020:<br>\n",
    "了解 API Request 用於動態網頁爬蟲的原理<br>\n",
    "能夠使用 API Request 撰寫動態網頁爬蟲<br>\n",
    "func : webdriver.Chrome,header,regex,dict,find\n",
    "<br><br>\n",
    "\n",
    "021:<br>\n",
    "能夠利用 Request + BeatifulSour 撰寫爬蟲，並存放到合適的資料結構<br>\n",
    "func : webdriver.Chrome,find,select,scroller\n",
    "<br><br>\n",
    "\n",
    "022:<br>\n",
    "能夠利用 selenium + BeautifulSoup 撰寫爬蟲，並存放到合適的資料結構<br>\n",
    "func : webdriver.Chrome,select,find,find_all,dateformat,dict\n",
    "<br><br>\n",
    "\n",
    "023:<br>\n",
    "學習利用Selenium模擬人為操作，更新動態網頁後爬取新聞內容<br>\n",
    "func : webdriver.Chrome,scroller,scroller end\n",
    "<br><br>\n",
    "\n",
    "024:<br>\n",
    "從104人力銀行網站爬取求職公司資訊<br>\n",
    "func : webdriver.Chrome,save file,next page,end page\n",
    "<br><br>\n",
    "\n",
    "025:ptt<br>\n",
    "熟悉單網站多網頁，透過列表將連結內的文章內容都爬下來<br>\n",
    "func : multi page crwal\n",
    "<br><br>\n",
    "\n",
    "026:<br>\n",
    "install scrapy\n",
    "<br><br>\n",
    "\n",
    "027:<br>\n",
    "scrapy : process_spider,open_spider,close_spider\n",
    "<br><br>\n",
    "\n",
    "028:<br>\n",
    "scrapy : main+參數\n",
    "<br><br>\n",
    "\n",
    "029:<br>\n",
    "利用外部參數實現較為彈性的爬蟲\n",
    "scrapy : scrapy crawl file -a border=stock\n",
    "<br><br>\n",
    "\n",
    "030:簡答<br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "031:<br>\n",
    "了解「檢查 HTTP 標頭檔」的反爬蟲機制<br>\n",
    "「檢查 HTTP 標頭檔」反爬蟲的因應策略\n",
    "<br><br>\n",
    "\n",
    "032:<br>\n",
    "了解「驗證碼機制」的反爬蟲機制<br>\n",
    "「驗證碼機制」反爬蟲的因應策略\n",
    "<br><br>\n",
    "\n",
    "033:<br>\n",
    "了解「登入權限機制」的反爬蟲機制<br>\n",
    "「登入權限機制」反爬蟲的因應策略\n",
    "<br><br>\n",
    "\n",
    "034:<br>\n",
    "了解「IP 黑/白名單」的反爬蟲機制<br>\n",
    "「IP 黑/白名單」反爬蟲的因應策略\n",
    "func :  webdriver.Chrome,select,find,find_all,next page,next disable,xpath,xpath check\n",
    "<br><br>\n",
    "\n",
    "035:<br>\n",
    "多線程爬蟲<br>\n",
    "func : 利用thread多工作業，可以加速爬取大量資料的速度\n",
    "<br><br>\n",
    "\n",
    "036:<br>\n",
    "非同步爬蟲<br>\n",
    "asyncio:多線程套件<br>\n",
    "async：加入asyncio的排程\n",
    "await：等待時不會理會這個動作,會多工執行其他部分\n",
    "aiohttp:跟網頁互動的模組，在asyncio中取代了requests套件\n",
    "<br>\n",
    "<br><br>\n",
    "\n",
    "037:簡答<br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "038:<br>\n",
    "beautifulsoup不支援xpath!<br>\n",
    "jieba參考資料：https://coderwall.com/p/38wtgw/jieba<br>\n",
    "TF-IDF參考資料：https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76a<br>\n",
    "stopword:https://github.com/goto456/stopwordsa<br>\n",
    "敏感詞庫下載：https://www.aitechclub.com/data-detail?data_id=18a<br>\n",
    "\n",
    "神奇的問題：<br>\n",
    "import nltk<br>\n",
    "nltk.download('stopwords')<br>\n",
    "想安裝nltk上的stopwords，跳出http 500,不管從nltk的console安裝還是從python安裝都一樣，\n",
    "改了proxy也沒用，後來突然從python安裝就成功了...完全不知所以然\n",
    "\n",
    "end\n",
    "\n",
    "<br><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
