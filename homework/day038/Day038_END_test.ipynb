{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 專題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本目標\n",
    "\n",
    "- 爬下文章，透過 jieba 等斷詞將文章拆解\n",
    "- 可以簡單的計算同樣文字出現的頻率或是透過 TFIDF 的統計方式計算\n",
    "- 將經常出現的 stop words 過濾掉之後對頻率進行排名\n",
    "- 將結果透過 wordcloud 文字雲的方式呈現\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 進階目標\n",
    "\n",
    "- 透過不同帳號，但是相同 IP 且政治用語的詞頻分佈類似的定位成網軍\n",
    "- 進一步分析帳號是否在特定期間 (e.g. 選舉) 有明顯的活動特性\n",
    "- 如果不同帳號但是政治用語的詞頻分佈類似，進一步判斷這些高頻率的單字是positive / negative 來歸納兩個帳號之間是否具有相同政治立場\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "{'a': 2}\n",
      "2\n",
      "{'a': 3}\n"
     ]
    }
   ],
   "source": [
    "data=dict({})\n",
    "print(bool(data))\n",
    "data['a']=2\n",
    "\n",
    "print(bool(data))\n",
    "#not data\n",
    "print(data)\n",
    "print(data['a'])\n",
    "if 'a' in data:\n",
    "    count=data['a']+1\n",
    "    tmp={}\n",
    "    tmp={'a':count}\n",
    "    data.update(tmp)\n",
    "    print(data)\n",
    "#print(len(data['b']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'sklearn.tree._criterion.array' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6fb183648adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# 5.    依據特徵資料訓練分類器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEnsemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomTreesEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m from ..tree import (DecisionTreeClassifier, DecisionTreeRegressor,\n\u001b[0m\u001b[1;32m     57\u001b[0m                     ExtraTreeClassifier, ExtraTreeRegressor)\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOUBLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDecisionTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_criterion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDepthFirstTreeBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/_criterion.cpython-37m-darwin.so\u001b[0m in \u001b[0;36minit sklearn.tree._criterion\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'sklearn.tree._criterion.array' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu May 31 11:21:44 2018\n",
    "Machine learning Example for「SMS Spam Collection Dataset」\n",
    "Database Link:\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset#spam.csv\n",
    " \n",
    "@author: Tommy Huang, chih.sheng.huang821@gmail.com\n",
    "\"\"\"\n",
    "\n",
    "# 1.\t首先我們先將資料匯入python內，我們會用到的pandas，pandas對處理這種文字資料滿好用的。\n",
    "import pandas as pd\n",
    "filepath='spam.csv'\n",
    "def readData_rawSMS(filepath):\n",
    "\tdata_rawSMS = pd.read_csv(filepath,usecols=[0,1],encoding='latin-1')\n",
    "\tdata_rawSMS.columns=['label','content']\n",
    "\treturn data_rawSMS\n",
    "data_rawSMS = readData_rawSMS(filepath)\n",
    "#########################################\n",
    "# kaggle的'spam.csv'將我範例非垃圾郵件的label寫的genuine改成ham\n",
    "# 所以如果要直接用我的程式，最簡單的方式就是ham改回genuine\n",
    "# 2019/02/27修改這段\n",
    "for i in range(data_rawSMS.shape[0]):\n",
    "    if data_rawSMS.iloc[i].label == 'ham':\n",
    "        data_rawSMS.iloc[i].label='genuine'\n",
    "###########################################\t\n",
    "#2.\t將資料分成Train和Test\n",
    "import numpy as np\n",
    "def Separate_TrainAndTest(data_rawSMS):\n",
    "    n=int(data_rawSMS.shape[0])\n",
    "    tmp_train=(np.random.rand(n)>=0.5)\n",
    "    return data_rawSMS.iloc[np.where(tmp_train==True)[0]], data_rawSMS.iloc[np.where(tmp_train==False)[0]]\n",
    "data_rawtrain,data_rawtest=Separate_TrainAndTest(data_rawSMS)\n",
    "\n",
    "#3. 從training data去著手算哪些「詞」重要。\n",
    "import re\n",
    "def generate_key_list(data_rawtrain, size_table=200,ignore=3):\n",
    "    dict_spam_raw = dict()\n",
    "    dict_genuine_raw = dict()\n",
    "    dict_IDF = dict()\n",
    "\n",
    "\t# ignore all other than letters.\n",
    "    for i in range(data_rawSMS.shape[0]):\n",
    "        finds = re.findall('[A-Za-z]+', data_rawSMS.iloc[i].content)\n",
    "        if data_rawSMS.iloc[i].label == 'spam':\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower() #英文轉成小寫\n",
    "                try:\n",
    "                    dict_spam_raw[find] = dict_spam_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,1)\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,0)\n",
    "        else:\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower()\n",
    "                try:\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,1)\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,0)\n",
    "\t\t\n",
    "        word_set = set()\n",
    "        for find in finds:\n",
    "            if len(find)<ignore: continue\n",
    "            find = find.lower()\n",
    "            if not(find in word_set):\n",
    "                try:\n",
    "                    dict_IDF[find] = dict_IDF[find] + 1\n",
    "                except:\t\n",
    "                    dict_IDF[find] = dict_IDF.get(find,1)\n",
    "            word_set.add(find)\n",
    "    word_df = pd.DataFrame(list(zip(dict_genuine_raw.keys(),dict_genuine_raw.values(),dict_spam_raw.values(),dict_IDF.values())))\n",
    "    word_df.columns = ['keyword','genuine','spam','IDF']\n",
    "    word_df['genuine'] = word_df['genuine'].astype('float')/data_rawtrain[data_rawtrain['label']=='genuine'].shape[0]\n",
    "    word_df['spam'] = word_df['spam'].astype('float')/data_rawtrain[data_rawtrain['label']=='spam'].shape[0]\n",
    "    word_df['IDF'] = np.log10(word_df.shape[0]/word_df['IDF'].astype('float'))\n",
    "    word_df['genuine_IDF'] = word_df['genuine']*word_df['IDF']\n",
    "    word_df['spam_IDF'] = word_df['spam']*word_df['IDF']\n",
    "    word_df['diff']=word_df['spam_IDF']-word_df['genuine_IDF']\n",
    "    selected_spam_key = word_df.sort_values('diff',ascending=False)  \n",
    "    keyword_dict = dict()\n",
    "    i = 0\n",
    "    for word in selected_spam_key.head(size_table).keyword:\n",
    "        keyword_dict.update({word.strip():i})\n",
    "        i+=1\n",
    "    return keyword_dict   \n",
    "# build a tabu list based on the training data\n",
    "size_table = 300                 # how many features are used to classify spam\n",
    "word_len_ignored = 3            # ignore those words shorter than this variable\n",
    "keyword_dict=generate_key_list(data_rawtrain, size_table, word_len_ignored)\n",
    "\n",
    "\n",
    "# 4.將Train資料和Test資料轉換成特徵向量\n",
    "def convert_Content(content, keyword_dict):\n",
    "\tm = len(keyword_dict)\n",
    "\tres = np.int_(np.zeros(m))\n",
    "\tfinds = re.findall('[A-Za-z]+', content)\n",
    "\tfor find in finds:\n",
    "\t\tfind=find.lower()\n",
    "\t\ttry:\n",
    "\t\t\ti = keyword_dict[find]\n",
    "\t\t\tres[i]=1\n",
    "\t\texcept:\n",
    "\t\t\tcontinue\n",
    "\treturn res\n",
    "def raw2feature(data_rawtrain,data_rawtest,keyword_dict):\n",
    "    n_train = data_rawtrain.shape[0]\n",
    "    n_test = data_rawtest.shape[0]\n",
    "    m = len(keyword_dict)\n",
    "    X_train = np.zeros((n_train,m));\n",
    "    X_test = np.zeros((n_test,m));\n",
    "    Y_train = np.int_(data_rawtrain.label=='spam')\n",
    "    Y_test = np.int_(data_rawtest.label=='spam')\n",
    "    for i in range(n_train):\n",
    "        X_train[i,:] = convert_Content(data_rawtrain.iloc[i].content, keyword_dict)\n",
    "    for i in range(n_test):\n",
    "        X_test[i,:] = convert_Content(data_rawtest.iloc[i].content, keyword_dict)\n",
    "        \n",
    "    return [X_train,Y_train],[X_test,Y_test]\n",
    "     \n",
    "Train,Test=raw2feature(data_rawtrain,data_rawtest,keyword_dict)\n",
    "\n",
    "\n",
    "# 5.\t依據特徵資料訓練分類器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB       \n",
    "def learn(Train):\n",
    "    model_NB = BernoulliNB()\n",
    "    model_NB.fit(Train[0], Train[1])\n",
    "    Y_hat_NB = model_NB.predict(Train[0])\n",
    "\n",
    "    model_RF = RandomForestClassifier(n_estimators=10, max_depth=None,\\\n",
    "                                 min_samples_split=2, random_state=0)\n",
    "    model_RF.fit(Train[0], Train[1])\n",
    "    Y_hat_RF = model_RF.predict(Train[0])\n",
    "    \n",
    "    n=np.size(Train[1])\n",
    "    print('Training Accuarcy NBclassifier : {:.2f}％'.format(sum(np.int_(Y_hat_NB==Train[1]))*100./n))\n",
    "    print('Training Accuarcy RF: {:.2f}％'.format(sum(np.int_(Y_hat_RF==Train[1]))*100./n))\n",
    "    return model_NB,model_RF\n",
    "# train the Random Forest and the Naive Bayes Model using training data\n",
    "model_NB,model_RF=learn(Train)\n",
    "\n",
    "# 6.依據訓練好的分類器，進行測試。\n",
    "def test(Test,model):\n",
    "    Y_hat = model.predict(Test[0])\n",
    "    n=np.size(Test[1])\n",
    "    print ('Testing Accuarcy: {:.2f}％ ({})'.format(sum(np.int_(Y_hat==Test[1]))*100./n,model.__module__))\n",
    "# Test Model using testing data\n",
    "test(Test,model_NB)\n",
    "test(Test,model_RF)\n",
    "\n",
    "#######\n",
    "def predictSMS(SMS,model,keyword_dict):\n",
    "    X = convert_Content(SMS, keyword_dict)\n",
    "    Y_hat = model.predict(X.reshape(1,-1))\n",
    "    if int(Y_hat) == 1:\n",
    "        print ('SPAM: {}'.format(SMS))\n",
    "    else:\n",
    "        print ('GENUINE: {}'.format(SMS))    \n",
    "\n",
    "inputstr='go to visit www.yahoo.com.tw, Buy one get one free, Hurry!'\n",
    "predictSMS(inputstr,model_NB,keyword_dict)\n",
    "\n",
    "inputstr=('Call back for anytime.')\n",
    "predictSMS(inputstr,model_NB,keyword_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
